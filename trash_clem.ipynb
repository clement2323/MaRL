{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marelle Board import\n",
    "* Getting players info dict => `MarelleBoard.players`\n",
    "* 3 phases : place, move and end => `MarelleBoard.phase`\n",
    "* Reset the board => `MarelleBoard.initialize_game()`\n",
    "* Printing the board => `MarelleBoard.print_board()`\n",
    "* Id:Action dict => `MarelleBoard.id_to_action`\n",
    "* Action:Id dict => `MarelleBoard.action_to_id`\n",
    "* Get the board state => `MarelleBoard.get_state()`\n",
    "* Play an action => `MarelleBoard.play_action(action_id, player)`\n",
    "* Get legal action ids => `MarelleBoard.get_legal_action_ids(player)`\n",
    "* Check if game ended (returns 0 if not ended or winning player id) : `MarelleBoard.check_if_end(player)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ge3pFEexRsNn"
   },
   "outputs": [],
   "source": [
    "from maRL_board import MarelleBoard\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch import optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Phase : place\n",
      "        Placed tokens : P1 : 0 / P2 : 0 \n",
      "        Tokens on board : P1 : 0 / P2 : 0 \n",
      "        0--------------0--------------0\n",
      "        |              |              |\n",
      "        |    0---------0---------0    |\n",
      "        |    |         |         |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |    |         |    |    |\n",
      "        0----0----0         0----0----0                 \n",
      "        |    |    |         |    |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |         |         |    |\n",
      "        |    0---------0---------0    |\n",
      "        |              |              |\n",
      "        0--------------0--------------0\n",
      "        \n",
      "\n",
      "        Phase : place\n",
      "        Placed tokens : P1 : 1 / P2 : 0 \n",
      "        Tokens on board : P1 : 1 / P2 : 0 \n",
      "        0--------------0--------------0\n",
      "        |              |              |\n",
      "        |    0---------0---------0    |\n",
      "        |    |         |         |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |    |         |    |    |\n",
      "        0----0----0         1----0----0                 \n",
      "        |    |    |         |    |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |         |         |    |\n",
      "        |    0---------0---------0    |\n",
      "        |              |              |\n",
      "        0--------------0--------------0\n",
      "        \n",
      "((0, 3), None)\n",
      "\n",
      "        Phase : place\n",
      "        Placed tokens : P1 : 1 / P2 : 1 \n",
      "        Tokens on board : P1 : 1 / P2 : 1 \n",
      "        0--------------0--------------0\n",
      "        |              |              |\n",
      "        |    0---------0---------0    |\n",
      "        |    |         |         |    |\n",
      "        |    |    2----0----0    |    |\n",
      "        |    |    |         |    |    |\n",
      "        0----0----0         1----0----0                 \n",
      "        |    |    |         |    |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |         |         |    |\n",
      "        |    0---------0---------0    |\n",
      "        |              |              |\n",
      "        0--------------0--------------0\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "board=MarelleBoard()\n",
    "board.print_board()\n",
    "a=board.get_legal_action_ids(1)[0]\n",
    "board.play_action(a,1)\n",
    "board.print_board()\n",
    "b=board.get_legal_action_ids(-1)[2]\n",
    "print(board.id_to_action[b])\n",
    "board.play_action(b,-1)\n",
    "board.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_dR83RzsOrc",
    "outputId": "cae599b2-ddc4-4ef9-ee33-7ac688b92f6c"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class MarelleGymEnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    def __init__(self):\n",
    "        super(MarelleGymEnv, self).__init__()    \n",
    "    \n",
    "    # Define action and observation space\n",
    "    # They must be gym.spaces objects\n",
    "        self.board = MarelleBoard()\n",
    "        self.action_space = spaces.Discrete(len(self.board.id_to_action))    # Example for using image as input:\n",
    "        self.observation_space = spaces.Discrete(len(self.board.get_state()))\n",
    "        self.list_move = []\n",
    "        self.current_player = 1\n",
    "        \n",
    "    def step(self, action): \n",
    "   \n",
    "        self.board.play_action(action,self.current_player)\n",
    "\n",
    "        observation=self.board.get_state()\n",
    "        done = self.board.check_if_end(self.current_player) != 0\n",
    "        reward = self.board.check_if_end(self.current_player)*self.current_player\n",
    "        self.list_move.append(action)\n",
    "        info = \"\"\n",
    "        self.current_player = self.board.get_opponent(self.current_player)\n",
    "\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    # Execute one time step within the environment\n",
    "  \n",
    "    def reset(self):\n",
    "        self.current_player = 1\n",
    "        self.list_move = []\n",
    "        self.board.initialize_game()\n",
    "\n",
    "        return self.board.get_state()\n",
    "    # Reset\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self.board.print_board()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une classse générale agent dont nos agents seront hérités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dim_observation, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.dim_observation = dim_observation\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=self.dim_observation, out_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=16, out_features=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=8, out_features=self.n_actions),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "    \n",
    "   # def select_action(self, state):\n",
    "    #    action = torch.multinomial(self.forward(state), 1)\n",
    "     #   return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, player_id, epsilon=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.n_action = len(self.env.board.id_to_action)\n",
    "        self.model = Model(len(self.env.board.get_state()), self.n_action)\n",
    "        self.gamma = 1\n",
    "        self.player_id = player_id\n",
    "        self.optimizer = torch.optim.Adam(self.model.net.parameters(), lr=0.01)\n",
    "        \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def _compute_returns(self, rewards):\n",
    "        \"\"\"Returns the cumulative discounted rewards at each time step\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        rewards : array\n",
    "            The array of rewards of one episode\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards at each time step\n",
    "            \n",
    "        Example\n",
    "        -------\n",
    "        for rewards=[1, 2, 3] this method outputs [1 + 2 * gamma + 3 * gamma**2, 2 + 3 * gamma, 3] \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def optimize_model(self, n_trajectories, adversaire):\n",
    "        \"\"\"Perform a gradient update using n_trajectories\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expectation card(D) in the formula above\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            The cumulative discounted rewards of each trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, n_trajectories, n_update, adversaire):\n",
    "        \"\"\"Training method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trajectories : int\n",
    "            The number of trajectories used to approximate the expected gradient\n",
    "        n_update : int\n",
    "            The number of gradient updates\n",
    "            \n",
    "        \"\"\"\n",
    "        rewards = []\n",
    "        for episode in range(n_update):\n",
    "            rewards.append(self.optimize_model(n_trajectories, adversaire))\n",
    "            #print(episode)\n",
    "            if (episode+1)%5 == 0:\n",
    "                print(f'Episode {episode + 1}/{n_update}: rewards {round(np.mean(rewards[-1]), 2)} +/- {round(np.std(rewards[-1]), 2)}')\n",
    "        \n",
    "        # Plotting\n",
    "        r = pd.DataFrame((itertools.chain(*(itertools.product([i], rewards[i]) for i in range(len(rewards))))), columns=['Epoch', 'Reward'])\n",
    "        sns.lineplot(x=\"Epoch\", y=\"Reward\", data=r, ci='sd');\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notre premier enfant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self, env, player_id):\n",
    "        super(RandomAgent, self).__init__(env, player_id)\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return(np.random.choice(self.env.board.get_legal_action_ids(self.player_id)))\n",
    "\n",
    "class BetterRandomAgent(Agent):\n",
    "    def __init__(self, env, player_id):\n",
    "        super(BetterRandomAgent, self).__init__(env, player_id)\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        legal_actions = self.env.board.get_legal_actions(self.player_id)\n",
    "        opponent_legal_actions = self.env.board.get_legal_actions(self.env.board.get_opponent(self.player_id))\n",
    "        \n",
    "        pos_to_block = []\n",
    "        for opponent_action in opponent_legal_actions:\n",
    "            # Check positions we should block\n",
    "            _, pos = opponent_action\n",
    "            if pos != None:\n",
    "                pos_to_block.append(pos)\n",
    "\n",
    "        blocker_action = None\n",
    "        for action in legal_actions:\n",
    "            # Check if possible to capture\n",
    "            pos, enemy_pos = action\n",
    "            if enemy_pos != None:\n",
    "                return self.env.board.action_to_id[action]\n",
    "            \n",
    "            # Check if possible to block\n",
    "            if self.env.board.phase == \"place\":\n",
    "                if pos in pos_to_block:\n",
    "                    blocker_action = action\n",
    "            \n",
    "            elif self.env.board.phase == \"move\":\n",
    "                a, b = pos\n",
    "                if a in pos_to_block or b in pos_to_block:\n",
    "                    blocker_action = action\n",
    "        \n",
    "        if blocker_action != None:\n",
    "            return self.env.board.action_to_id[blocker_action]\n",
    "        \n",
    "        return(np.random.choice(self.env.board.get_legal_action_ids(self.player_id)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Phase : place\n",
      "        Placed tokens : P1 : 0 / P2 : 0 \n",
      "        Tokens on board : P1 : 0 / P2 : 0 \n",
      "        0--------------0--------------0\n",
      "        |              |              |\n",
      "        |    0---------0---------0    |\n",
      "        |    |         |         |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |    |         |    |    |\n",
      "        0----0----0         0----0----0                 \n",
      "        |    |    |         |    |    |\n",
      "        |    |    0----0----0    |    |\n",
      "        |    |         |         |    |\n",
      "        |    0---------0---------0    |\n",
      "        |              |              |\n",
      "        0--------------0--------------0\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [ 0.1514, -0.0831, -0.2308, -0.3355, -0.1920, -0.2729, -0.1000, -0.0336,\n",
       "          -0.0391, -0.1113, -0.1558, -0.0413, -0.0040, -0.1283, -0.1122,  0.0483],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
       "         [    nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "              nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([    nan,     nan,     nan,     nan, -0.2492,     nan,     nan,     nan],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([nan, nan, nan,  ..., nan, nan, nan], requires_grad=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einstein.env.board.print_board()\n",
    "[p for p in einstein.model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5/100: rewards -0.66 +/- 0.75\n",
      "Episode 10/100: rewards -0.48 +/- 0.88\n",
      "Episode 15/100: rewards -0.3 +/- 0.95\n",
      "Episode 20/100: rewards -0.11 +/- 0.99\n",
      "Episode 25/100: rewards 0.13 +/- 0.99\n",
      "Episode 30/100: rewards 0.14 +/- 0.99\n",
      "Episode 35/100: rewards 0.19 +/- 0.98\n",
      "Episode 40/100: rewards 0.27 +/- 0.96\n",
      "Episode 45/100: rewards 0.25 +/- 0.97\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-5093bdd19a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpiccolo_brain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBetterRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0meinstein\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversaire\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiccolo_brain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#TO DO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#random agent, random qui prend quand il faut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-112b3150ea2a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_trajectories, n_update, adversaire)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversaire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m#print(episode)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-1b7f1482bc2a>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(self, n_trajectories, adversaire)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mt_all_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mt_legal_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_all_moves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlegal_move\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlegal_move\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0maction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_legal_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "env=MarelleGymEnv()\n",
    "einstein=Reinforce(env,1)\n",
    "piccolo=RandomAgent(env,-1)\n",
    "piccolo_brain=BetterRandomAgent(env,-1)\n",
    "\n",
    "einstein.train(1000,100, adversaire = piccolo_brain)\n",
    "#TO DO\n",
    "#random agent, random qui prend quand il faut\n",
    "#learning rate ? config\n",
    "#calibrage rewards \n",
    "#CNN avec le bon\n",
    "#reward à 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce(Agent):\n",
    "    def __init__(self,env, player_id):\n",
    "        super(Reinforce, self).__init__(env, player_id)\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s): #checker legal move + argmax\n",
    "        legal_moves = self.env.board.get_legal_action_ids(self.player_id)\n",
    "        t_all_moves=np.array(self.model(s).detach())\n",
    "        t_legal_moves =[t_all_moves[legal_move] for legal_move in legal_moves]\n",
    "        argm = np.argmax(t_legal_moves)\n",
    "        action = legal_moves[argm]\n",
    "\n",
    "        return(action) \n",
    "        \n",
    "    def _compute_returns(self, rewards):\n",
    "        \n",
    "        returns=0\n",
    "        for i in range(len(rewards)):\n",
    "            returns=self.gamma*returns+rewards[-(i+1)]\n",
    "        return(returns)\n",
    "               \n",
    "    def optimize_model(self, n_trajectories, adversaire): #adversaire on met dedans l'agent qui jouera contre\n",
    "      \n",
    "        reward_trajectories=[]\n",
    "        list_sum_proba=[]\n",
    "        \n",
    "        #Here I compute n_trajectories trajectories in order to calculate the MonteCarlo estimate of the J function\n",
    "        for i in range(n_trajectories):\n",
    "            done = False\n",
    "            rewards=[]\n",
    "\n",
    "            state=self.env.reset()\n",
    "            state=torch.tensor(state, dtype=torch.float)\n",
    "            \n",
    "            sum_lprob=0\n",
    "            while not done:\n",
    "                \n",
    "                \n",
    "                legal_moves = self.env.board.get_legal_action_ids(self.player_id)\n",
    "                t_all_moves = self.model(state)\n",
    "                t_legal_moves = torch.tensor([t_all_moves[legal_move] for legal_move in legal_moves])\n",
    "                action_id = int(torch.multinomial(t_legal_moves, 1))\n",
    "                action = legal_moves[action_id]\n",
    "                \n",
    "                sum_lprob+= t_all_moves[action].log()\n",
    "                state, reward_p1, done, info =self.env.step(action)\n",
    "                \n",
    "                \n",
    "                #au tour de l'adversaire\n",
    "                if not done:\n",
    "                    action=adversaire.act(state)\n",
    "                    state, reward_p2, done, info = self.env.step(action)\n",
    "                    rewards.append(reward_p1-reward_p2)\n",
    "                else : \n",
    "                    rewards.append(reward_p1)\n",
    "                    \n",
    "                state=torch.tensor(state, dtype=torch.float)\n",
    "            \n",
    "            list_sum_proba.append(sum_lprob)\n",
    "            reward_trajectories.append(self._compute_returns(rewards))\n",
    "        \n",
    "        loss=0\n",
    "        for i in range(len(list_sum_proba)):\n",
    "            loss+=-list_sum_proba[i]*reward_trajectories[i]\n",
    "        \n",
    "        loss=loss/len(list_sum_proba)\n",
    "       \n",
    "        # The following lines take care of the gradient descent step for the variable loss\n",
    "        # Discard previous gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Compute the gradient \n",
    "        loss.backward()\n",
    "        # Do the gradient descent step\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return reward_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jouons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Getting players info dict => `MarelleBoard.players`\n",
    "* 3 phases : place, move and end => `MarelleBoard.phase`\n",
    "* Reset the board => `MarelleBoard.initialize_game()`\n",
    "* Printing the board => `MarelleBoard.print_board()`\n",
    "* Id:Action dict => `MarelleBoard.id_to_action`\n",
    "* Action:Id dict => `MarelleBoard.action_to_id`\n",
    "* Get the board state => `MarelleBoard.get_state()`\n",
    "* Play an action => `MarelleBoard.play_action(action_id, player)`\n",
    "* Get legal action ids => `MarelleBoard.get_legal_action_ids(player)`\n",
    "* Check if game ended (returns 0 if not ended or winning player id) : `MarelleBoard.check_if_end()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import progressbar as pb\n",
    "import numpy as np\n",
    "\n",
    "compteur_partie=0\n",
    "compteur_piccolo=0\n",
    "compteur_gabrielle=0\n",
    "\n",
    "n_partie = 100\n",
    "n_tour = 500\n",
    "n_pion_p1 = np.zeros((n_partie,2*n_tour))\n",
    "n_pion_p2 = np.zeros((n_partie,2*n_tour))\n",
    "list_n_coup_joue=[]\n",
    "\n",
    "for j in range(n_partie): #(n parties)\n",
    "    marelle_env = MarelleGymEnv()\n",
    "    piccolo= RandomAgent(marelle_env,1)\n",
    "    gabrielle = RandomAgent(marelle_env,-1)\n",
    "\n",
    "    compteur_partie+=1\n",
    "    pion_p1 = np.zeros(2*n_tour)\n",
    "    pion_p2 = np.zeros(2*n_tour)\n",
    "    n_coup_joue=0\n",
    "    for i in range (n_tour):\n",
    "\n",
    "        s=marelle_env.board.get_state()\n",
    "        a_p1 = piccolo.act(s,train = False)\n",
    "        s, reward, done, info = marelle_env.step(a_p1)\n",
    "        pion_p1[i]=marelle_env.board.players[1][\"tokens_on_board\"]\n",
    "        pion_p2[i] = marelle_env.board.players[-1][\"tokens_on_board\"]\n",
    "        n_coup_joue+=1\n",
    "        if done:\n",
    "            #print('Piccolo win fatality')\n",
    "            compteur_piccolo+=1\n",
    "            list_n_coup_joue.append(n_coup_joue)\n",
    "            break\n",
    "\n",
    "        #marelle_env.render()\n",
    "\n",
    "        a_p2 = gabrielle.act(s,train = False)\n",
    "        s, reward, done, info = marelle_env.step(a_p2)\n",
    "        pion_p1[i] = marelle_env.board.players[1][\"tokens_on_board\"]\n",
    "        pion_p2[i] = marelle_env.board.players[-1][\"tokens_on_board\"]\n",
    "        n_coup_joue+=1\n",
    "        if done :\n",
    "            #print('gabrielle win encore..')\n",
    "            compteur_gabrielle+=1\n",
    "            list_n_coup_joue.append(n_coup_joue)\n",
    "            break\n",
    "    \n",
    "    n_pion_p1[j]= pion_p1\n",
    "    n_pion_p2[j]= pion_p2    \n",
    "        #marelle_env.render()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 47 53\n",
      "81.41\n",
      "64.34160318176724\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(compteur_partie, compteur_piccolo, compteur_gabrielle)\n",
    "print(np.mean(list_n_coup_joue))\n",
    "print(np.std(list_n_coup_joue))\n",
    "plt.plot(np.mean(n_pion_p1,axis = 0)[:100])\n",
    "plt.plot(np.mean(n_pion_p2,axis = 0)[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "def boucles(n_partie,n_tour):\n",
    "    for j in range(n_partie): #(n parties)\n",
    "        marelle_env = MarelleGymEnv()\n",
    "        piccolo= RandomAgent(marelle_env.board,1)\n",
    "        gabrielle = RandomAgent(marelle_env.board,-1)\n",
    "\n",
    "        for i in range (n_tour):\n",
    "\n",
    "            s=marelle_env.board.get_state()\n",
    "            a_p1 = piccolo.act(s,train = False)\n",
    "            s, reward, done, info = marelle_env.step(a_p1)\n",
    "            if done:\n",
    "                #print('Piccolo win fatality')\n",
    "                break\n",
    "\n",
    "            a_p2 = gabrielle.act(s,train = False)\n",
    "            s, reward, done, info = marelle_env.step(a_p2)\n",
    "            if done :\n",
    "                #print('gabrielle win encore..')\n",
    "                break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         471080012 function calls (384196460 primitive calls) in 132.630 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      200    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(concatenate)\n",
      "     7255    0.005    0.000   66.159    0.009 <ipython-input-13-c9cd4e13515a>:11(act)\n",
      "      200    0.000    0.000    0.000    0.000 <ipython-input-13-c9cd4e13515a>:2(__init__)\n",
      "      200    0.000    0.000    0.000    0.000 <ipython-input-14-18967b4e80bd>:2(__init__)\n",
      "     7255    0.020    0.000   66.154    0.009 <ipython-input-14-18967b4e80bd>:6(learned_act)\n",
      "        1    0.024    0.024  132.630  132.630 <ipython-input-18-230384b5c51d>:2(boucles)\n",
      "     7255    0.028    0.000   66.194    0.009 <ipython-input-7-abcc0e7cf1ff>:15(step)\n",
      "      100    0.001    0.000    0.140    0.001 <ipython-input-7-abcc0e7cf1ff>:4(__init__)\n",
      "        1    0.000    0.000  132.630  132.630 <string>:1(<module>)\n",
      "      200    0.000    0.000    0.001    0.000 _asarray.py:16(asarray)\n",
      "     7255    0.026    0.000    0.058    0.000 _dtype.py:319(_name_get)\n",
      "      200    0.000    0.000    0.002    0.000 _methods.py:44(_any)\n",
      "      400    0.001    0.000    0.001    0.000 _ufunc_config.py:139(geterr)\n",
      "      400    0.001    0.000    0.002    0.000 _ufunc_config.py:39(seterr)\n",
      "      200    0.000    0.000    0.002    0.000 _ufunc_config.py:441(__enter__)\n",
      "      200    0.000    0.000    0.001    0.000 _ufunc_config.py:446(__exit__)\n",
      "      200    0.000    0.000    0.000    0.000 abc.py:137(__instancecheck__)\n",
      "      200    0.000    0.000    0.000    0.000 contextlib.py:58(_recreate_cm)\n",
      "      200    0.016    0.000    0.019    0.000 contextlib.py:71(inner)\n",
      "74860128/143136   55.657    0.000  127.703    0.001 copy.py:132(deepcopy)\n",
      " 41652576    3.190    0.000    3.190    0.000 copy.py:190(_deepcopy_atomic)\n",
      " 16031232   15.861    0.000   55.438    0.000 copy.py:219(_deepcopy_tuple)\n",
      " 16031232    7.347    0.000   38.523    0.000 copy.py:220(<listcomp>)\n",
      "12166560/143136   16.172    0.000  125.568    0.001 copy.py:236(_deepcopy_dict)\n",
      " 12452832    5.142    0.000    7.050    0.000 copy.py:252(_keep_alive)\n",
      "286272/143136    0.857    0.000  126.831    0.001 copy.py:268(_reconstruct)\n",
      "   572544    0.158    0.000    0.597    0.000 copy.py:273(<genexpr>)\n",
      "   286272    0.185    0.000    0.243    0.000 copyreg.py:87(__newobj__)\n",
      "      200    0.001    0.000    0.060    0.000 discrete.py:13(__init__)\n",
      "    11010    0.009    0.000    0.026    0.000 graph.py:1242(edges)\n",
      "      100    0.000    0.000    0.000    0.000 graph.py:290(__init__)\n",
      "     2400    0.002    0.000    0.002    0.000 graph.py:467(add_node)\n",
      "  2057656    0.875    0.000    1.187    0.000 graph.py:656(nodes)\n",
      "     3200    0.003    0.000    0.004    0.000 graph.py:820(add_edge)\n",
      "     7255    0.029    0.000   65.921    0.009 maRL_board.py:111(play_action)\n",
      "     7255    0.080    0.000   65.880    0.009 maRL_board.py:120(get_legal_action_ids)\n",
      "    11007    0.150    0.000    0.354    0.000 maRL_board.py:134(get_state)\n",
      "     1800    0.013    0.000   26.285    0.015 maRL_board.py:141(place_token_action)\n",
      "     3600    0.495    0.000   52.521    0.015 maRL_board.py:157(place_token_legal_actions)\n",
      "      100    0.048    0.000    0.075    0.001 maRL_board.py:16(__init__)\n",
      "     5455    0.042    0.000   39.596    0.007 maRL_board.py:176(move_token_action)\n",
      "    10910    1.271    0.000   79.078    0.007 maRL_board.py:199(move_token_legal_actions)\n",
      "   143136    0.381    0.000    0.461    0.000 maRL_board.py:228(check_if_capture)\n",
      "     7255    0.007    0.000    0.007    0.000 maRL_board.py:261(change_phase_if_needed)\n",
      "    21765    0.009    0.000    0.009    0.000 maRL_board.py:265(check_if_end)\n",
      "   356438    0.039    0.000    0.039    0.000 maRL_board.py:282(get_opponent)\n",
      "      100    0.004    0.000    0.012    0.000 maRL_board.py:55(initialize_game)\n",
      "      200    0.000    0.000    0.000    0.000 multiarray.py:145(concatenate)\n",
      "    14510    0.012    0.000    0.019    0.000 numerictypes.py:293(issubclass_)\n",
      "     7255    0.012    0.000    0.032    0.000 numerictypes.py:365(issubdtype)\n",
      "      200    0.001    0.000    0.002    0.000 random.py:681(getrandbits)\n",
      "   363330    0.281    0.000    0.283    0.000 reportviews.py:1116(__iter__)\n",
      "   143136    0.073    0.000    0.073    0.000 reportviews.py:170(__getstate__)\n",
      "   143136    0.042    0.000    0.042    0.000 reportviews.py:173(__setstate__)\n",
      "  2057656    0.312    0.000    0.312    0.000 reportviews.py:176(__init__)\n",
      "    34817    0.013    0.000    0.021    0.000 reportviews.py:183(__iter__)\n",
      "  2308275    0.382    0.000    0.382    0.000 reportviews.py:186(__getitem__)\n",
      "    11010    0.013    0.000    0.017    0.000 reportviews.py:992(__init__)\n",
      "      200    0.019    0.000    0.059    0.000 seeding.py:11(np_random)\n",
      "      200    0.001    0.000    0.003    0.000 seeding.py:21(hash_seed)\n",
      "      200    0.000    0.000    0.005    0.000 seeding.py:45(create_seed)\n",
      "      400    0.003    0.000    0.004    0.000 seeding.py:69(_bigint_from_bytes)\n",
      "      200    0.000    0.000    0.000    0.000 seeding.py:80(_int_list_from_bigint)\n",
      "      200    0.000    0.000    0.059    0.000 space.py:21(seed)\n",
      "      200    0.001    0.000    0.060    0.000 space.py:9(__init__)\n",
      "   286272    0.059    0.000    0.059    0.000 {built-in method __new__ of type object at 0x109e2a568}\n",
      "      200    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method _hashlib.openssl_sha512}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        1    0.000    0.000  132.630  132.630 {built-in method builtins.exec}\n",
      "   572544    0.186    0.000    0.186    0.000 {built-in method builtins.getattr}\n",
      "   297282    0.061    0.000    0.061    0.000 {built-in method builtins.hasattr}\n",
      "115940160    7.678    0.000    7.678    0.000 {built-in method builtins.id}\n",
      "   429408    0.089    0.000    0.089    0.000 {built-in method builtins.isinstance}\n",
      "   451173    0.079    0.000    0.079    0.000 {built-in method builtins.issubclass}\n",
      "    34817    0.007    0.000    0.007    0.000 {built-in method builtins.iter}\n",
      "     1200    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "      200    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
      "      200    0.001    0.000    0.001    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "      800    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "      400    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "      400    0.003    0.000    0.003    0.000 {built-in method posix.urandom}\n",
      "   286272    0.365    0.000    0.438    0.000 {method '__reduce_ex__' of 'object' objects}\n",
      " 13049044    1.137    0.000    1.137    0.000 {method 'append' of 'list' objects}\n",
      "     7255    0.196    0.000    0.254    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "      200    0.001    0.000    0.001    0.000 {method 'digest' of '_hashlib.HASH' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      200    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "      400    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "145286240   12.246    0.000   12.246    0.000 {method 'get' of 'dict' objects}\n",
      " 12177570    1.106    0.000    1.106    0.000 {method 'items' of 'dict' objects}\n",
      "      200    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "      200    0.006    0.000    0.008    0.000 {method 'seed' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   148836    0.090    0.000    0.090    0.000 {method 'update' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run(\"boucles(100,130)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "maRL_environment",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
