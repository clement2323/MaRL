{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marelle Board import\n",
    "* Getting players info dict => `MarelleBoard.players`\n",
    "* 3 phases : place, move and end => `MarelleBoard.phase`\n",
    "* Reset the board => `MarelleBoard.initialize_game()`\n",
    "* Printing the board => `MarelleBoard.print_board()`\n",
    "* Id:Action dict => `MarelleBoard.id_to_action`\n",
    "* Action:Id dict => `MarelleBoard.action_to_id`\n",
    "* Get the board state => `MarelleBoard.get_state()`\n",
    "* Play an action => `MarelleBoard.play_action(action_id, player)`\n",
    "* Get legal action ids => `MarelleBoard.get_legal_action_ids(player)`\n",
    "* Check if game ended (returns 0 if not ended or winning player id) : `MarelleBoard.check_if_end(player)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ge3pFEexRsNn"
   },
   "outputs": [],
   "source": [
    "import marl_env\n",
    "import marl_agents\n",
    "import marl_models\n",
    "# importlib.reload(marl_env)\n",
    "# importlib.reload(marl_agents)\n",
    "# importlib.reload(marl_models)\n",
    "\n",
    "from marl_env import MarelleBoard, MarelleGymEnv, MarelleGame\n",
    "from marl_models import FCModel\n",
    "from marl_agents import RandomAgent, BetterRandomAgent, Reinforce\n",
    "\n",
    "import progressbar as pb\n",
    "import numpy as np \n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './utils')\n",
    "from cliffwalk import CliffWalk\n",
    "env = CliffWalk(proba_succ=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    env,\n",
    "    n_epochs,\n",
    "    n_trajectories,\n",
    "    trained_agent,\n",
    "    opponent_agent,\n",
    "    log_training=False,\n",
    "    save_model_freq=50):\n",
    "\n",
    "    if log_training:\n",
    "        wandb.init(project=\"marl\")\n",
    "        wandb.config.n_trajectories = n_trajectories\n",
    "        wandb.n_epochs = n_epochs\n",
    "\n",
    "        # Trained agent\n",
    "        wandb.config.trained_agent = trained_agent.__class__.__name__\n",
    "        wandb.config.incentivize_captures = trained_agent.incentivize_captures\n",
    "        wandb.config.punish_opponent_captures = trained_agent.punish_opponent_captures\n",
    "        wandb.config.lr = trained_agent.lr\n",
    "        wandb.config.optimizer_name = trained_agent.optimizer.__class__.__name__\n",
    "        \n",
    "        # Opponent agent\n",
    "        wandb.config.opponent_agent = opponent_agent.__class__.__name__\n",
    "\n",
    "        # Model to watch metrics from\n",
    "        if trained_agent.model != None :\n",
    "            wandb.watch(trained_agent.model)\n",
    "        \n",
    "    trained_agent.train(n_trajectories, n_epochs, opponent_agent, log_training, save_model_freq)\n",
    "\n",
    "def load_model(model_name, run_id, model):\n",
    "    wandb.restore(model_name, run_path=f'clement-guillo/marl/{run_id}',root= \"models/temp/\")\n",
    "    model.load_state_dict(torch.load(f'models/temp/{model_name}'))\n",
    "    os.remove(f'models/temp/{model_name}')\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test CliffWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(marl_env)\n",
    "importlib.reload(marl_agents)\n",
    "importlib.reload(marl_models)\n",
    "from marl_env import MarelleBoard, MarelleGymEnv, MarelleGame\n",
    "from marl_models import FCModel\n",
    "from marl_agents import RandomAgent, BetterRandomAgent, Reinforce, Reinforce_place, Reinforce_cliff\n",
    "\n",
    "env = CliffWalk(proba_succ=0.98)\n",
    "model = FCModel(2, env.Na)\n",
    "print(\"go\")\n",
    "intel=Reinforce_cliff(env, model, lr=0.001)\n",
    "intel.train(100, 10, None, None, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "o=intel.model(torch.tensor([prev_state,state],dtype = torch.float))\n",
    "nn.Softmax(dim=0)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 0.6, False, {})\n",
      "+-----------------------+\n",
      "| : : : : : : : : : : : |\n",
      "| : : : : : : : : : : : |\n",
      "|\u001b[43m_\u001b[0m: : : : : : : : : : : |\n",
      "|S:x:x:x:x:x:x:x:x:x:x:G|\n",
      "+-----------------------+\n",
      "  (up)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(env.step(3))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prev_state=intel.env.state\n",
    "state=intel.env.state\n",
    "for i in range(10):\n",
    "    print(prev_state,state)\n",
    "    action = intel.learned_act(prev_state,state)\n",
    "    prev_state=state\n",
    "    state, reward, done, _ = intel.env.step(action)\n",
    "    intel.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Marelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(marl_env)\n",
    "importlib.reload(marl_agents)\n",
    "importlib.reload(marl_models)\n",
    "from marl_env import MarelleBoard, MarelleGymEnv, MarelleGame\n",
    "from marl_models import FCModel\n",
    "from marl_agents import RandomAgent, BetterRandomAgent, Reinforce, Reinforce_place\n",
    "\n",
    "env = MarelleGymEnv()\n",
    "\n",
    "einstein_model = FCModel(len(env.board.get_state()), len(env.board.id_to_action))\n",
    "einstein_place_model = FCModel(len(env.board.get_state()), 566)\n",
    "\n",
    "einstein=Reinforce(env, 1, einstein_model, lr=0.005, incentivize_captures=False, punish_opponent_captures=False)\n",
    "einstein=Reinforce_place(env, 1, einstein_place_model, lr=0.005, incentivize_captures=True, punish_opponent_captures=True)\n",
    "\n",
    "\n",
    "piccolo=BetterRandomAgent(env, -1)\n",
    "piccolo_dumb=RandomAgent(env,-1)\n",
    "\n",
    "print(\"training\")\n",
    "train_agent(\n",
    "    env=env,\n",
    "    n_epochs=1000,\n",
    "    n_trajectories=500,\n",
    "    trained_agent=einstein,\n",
    "    #opponent_agent=piccolo,\n",
    "    opponent_agent=piccolo_dumb,\n",
    "    log_training=True,\n",
    "    save_model_freq=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_s=env.board.get_state()\n",
    "vec_s_2=[]\n",
    "for s in vec_s:\n",
    "    if s==0:\n",
    "        vec_s_2.append(-3)\n",
    "    else:\n",
    "        vec_s_2.append(s)\n",
    "\n",
    "s=vec_s_2\n",
    "mat_state=np.ones((7,7))*(-3)\n",
    "mat_state[3,4]=s[0]\n",
    "mat_state[2,4]=s[1]\n",
    "mat_state[2,3]=s[2]\n",
    "mat_state[2,2]=s[3]\n",
    "\n",
    "mat_state[3,2]=s[4]\n",
    "mat_state[4,2]=s[5]\n",
    "mat_state[4,3]=s[6]\n",
    "mat_state[4,4]=s[7]\n",
    "\n",
    "mat_state[3,5]=s[8]\n",
    "mat_state[1,5]=s[9]\n",
    "mat_state[1,3]=s[10]\n",
    "mat_state[1,1]=s[11]\n",
    "\n",
    "mat_state[3,1]=s[12]\n",
    "mat_state[3,3]=s[13]\n",
    "mat_state[3,5]=s[14]\n",
    "mat_state[5,5]=s[15]\n",
    "\n",
    "mat_state[3,6]=s[16]\n",
    "mat_state[0,6]=s[17]\n",
    "mat_state[0,3]=s[18]\n",
    "mat_state[0,0]=s[19]\n",
    "\n",
    "mat_state[3,0]=s[20]\n",
    "mat_state[6,0]=s[21]\n",
    "mat_state[6,3]=s[22]\n",
    "mat_state[6,6]=s[23]\n",
    "\n",
    "print(mat_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#agent fort en place\n",
    "# avec un conv net 7x7 board +conv dans agent\n",
    "# ne filtrer que les places ? -> non les licitesq moves servent à ça\n",
    "#reward = difference de jeton à la fin de la phase changer le step et le reward associé ?\n",
    "#mon reward \"place_phase\" nécessite des prises peut etre le faire jouer contre l'aléatoire?\n",
    "#créer un env particulier pour l'agent place pour pas triturer le done, plus propre, en parler à Tim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restauration Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(marl_env)\n",
    "importlib.reload(marl_agents)\n",
    "importlib.reload(marl_models)\n",
    "from marl_env import MarelleBoard, MarelleGymEnv, MarelleGame\n",
    "from marl_models import FCModel\n",
    "from marl_agents import RandomAgent, BetterRandomAgent, Reinforce, Reinforce_place\n",
    "\n",
    "env = MarelleGymEnv()\n",
    "einstein_place_model = load_model('model_200_1000.pt','3jdqnjih',FCModel(len(env.board.get_state()),566))\n",
    "einstein = Reinforce_place(env, 1, einstein_place_model, lr=0.01, incentivize_captures=True, punish_opponent_captures=True)\n",
    "\n",
    "\n",
    "gabrielle = RandomAgent(env, -1)\n",
    "piccolo = BetterRandomAgent(env, 1)\n",
    "game = MarelleGame(env=env, player1 = einstein, player2=gabrielle, clear_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#evaluate agent, faire jouer un agent entrainé\n",
    "#couleur sur l'affichage, ou vidéo animé (interface click bouton ?)\n",
    "#Paramétrer la fréquence d'enregistrement\n",
    "# récupérer les fichiers sauvés.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "env = MarelleGymEnv()\n",
    "\n",
    "einstein_model = FCModel(len(env.board.get_state()), len(env.board.id_to_action))\n",
    "\n",
    "einstein=Reinforce(env, 1, einstein_model, lr=0.005, incentivize_captures=False, punish_opponent_captures=False)\n",
    "\n",
    "piccolo=BetterRandomAgent(env, -1)\n",
    "\n",
    "\n",
    "cProfile.run(\"\"\"train_agent(\n",
    "    env=env,\n",
    "    n_epochs=10,\n",
    "    n_trajectories=100,\n",
    "    trained_agent=einstein,\n",
    "    opponent_agent=piccolo,\n",
    "    log_training=False\n",
    ")\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "maRL_environment",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
